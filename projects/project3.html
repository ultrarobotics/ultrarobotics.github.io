<div class="project-slide active" id="project3">
  <div class="video-container">
    <iframe 
      src="https://www.youtube.com/embed/8Zao18r7ETY?autoplay=1&mute=1&loop=1&playlist=8Zao18r7ETY&controls=1&modestbranding=1&rel=0" 
      title="Shelf Scanning Robot Project Video" 
      frameborder="0" 
      allow="autoplay; encrypted-media" 
      allowfullscreen>
    </iframe>
  </div>
  <div class="thesis-container">
    <div class="thesis-text">
      <h4>Multidisciplinary Group Project: Shelf Scanning Robot [Group Project]</h4>
      <p class="project-date"><strong>Duration:</strong> 04-2023 – 07-2023</p>
      <p> This project was the capstone of my first year—a group effort where each member focused on a different discipline within robotics. We developed a robot capable of scanning store shelves and removing misplaced items. I led the software architecture and motion planning aspects of the project. </p>
      
      <h5>Software Architecture & System Integration</h5>
      <ul>
        <li>Designed and implemented a modular ROS-based system architecture, ensuring efficient communication between software components.</li>
        <li>Integrated various subsystems, including object detection (vision), autonomous navigation, and robotic manipulation.</li>
        <li>Developed ROS nodes to manage task sequencing, enabling smooth execution of scanning, object detection, and pick-and-place tasks.</li>
      </ul>
      
      <h5>Motion Planning & Control</h5>
      <ul>
        <li>Implemented motion planning algorithms using <strong>MoveIt</strong> for robotic arm manipulation, allowing the robot to interact with shelf items.</li>
        <li>Developed trajectory execution scripts to coordinate <strong>pick-and-place movements</strong> based on object locations detected by the vision system (AprilTags).</li>
        <li>Designed motion sequences for product retrieval, misplaced item removal, and customer engagement gestures.</li>
      </ul>
      
      <h5>Simulation & Real-World Validation</h5>
      <ul>
        <li>Utilized a simulated testing environment in <strong>Gazebo</strong> to validate arm trajectories before real-world deployment.</li>
        <li>Tested and fine-tuned robotic motions in real-world conditions to ensure reliable performance in store environments.</li>
      </ul>
      
      <h5>Results</h5>
      <ul>
        <li><strong>Successfully developed and demonstrated a functional shelf-scanning robot capable of detecting and removing misplaced items in a demo store environment.</strong></li>
      </ul>
    </div>
  </div>
  <div class="video-container">
    <iframe 
      src="https://www.youtube.com/embed/YQHnjv0ZaXk?autoplay=1&mute=1&loop=1&playlist=YQHnjv0ZaXk&controls=1&modestbranding=1&rel=0" 
      title="Paper Reproducibility: Transporter Networks Video" 
      frameborder="0" 
      allow="autoplay; encrypted-media" 
      allowfullscreen>
    </iframe>
  </div>
  <div class="thesis-container">
    <div class="thesis-text">
      <h4>Deep Learning Paper Reproducibility: Transporter Networks [Group Project]</h4>
      <p class="project-date"><strong>Duration:</strong> 02-2023 – 04-2023</p>
      <p>As part of a team for this small-scale project, we worked on reproducing results from the research paper <a href="https://transporternets.github.io/" target="_blank">Transporter Networks: Rearranging the Visual World for Robotic Manipulation</a>. This involved setting up and running the codebase, training the models, and evaluating whether we could replicate the reported results. Additionally, we modified the codebase to assess how more challenging tasks influence the approach.</p>
      
      <h5>Key Contributions</h5>
      <ul>
        <li>Set up and configured the Google Cloud environment for model training and evaluation.</li>
        <li>Implemented modifications to the data augmentation pipeline, creating a new stacking task with increased complexity.</li>
        <li>Ran training and evaluation experiments, achieving a <strong>97% success rate</strong> on the original task and <strong>86% on an extended 10-block version</strong>.</li>
        <li>Investigated discrepancies between our reproduced results and the original paper, identifying potential sources of variation.</li>
      </ul>
      
      <h5>Results</h5>
      <ul>
        <li>Successfully reproduced the core results of the Transporter Networks paper.</li>
        <li>Demonstrated robustness of the model by extending the task complexity.</li>
      </ul>
    </div>
  </div>

    <div class="thesis-container">
      <div class="thesis-text">
        <h4>Seminar Computer Vision Project: Bridging the Sim2Real Gap in Robotic Vision with GANs [Group Project]</h4>
        <p class="project-date"><strong>Duration:</strong> 04-2023 – 07-2023</p>
        <p>This small-scale project explored how Generative Adversarial Networks (GANs), specifically <a href="https://junyanz.github.io/CycleGAN/" target="_blank">CycleGAN</a>, can transform simulated images of retail products into realistic ones—reducing the Sim2Real gap in robotic vision. The approach aimed to enhance robotic manipulation tasks by enabling effective training in simulation with minimal need for real-world retraining.</p>
        
        <h5>Key Contributions</h5>
        <ul>
          <li>Developed and trained a CycleGAN model for unpaired image-to-image translation between simulated and real environments.</li>
          <li>Implemented evaluation metrics, including the Inception Score (IS) and Fréchet Inception Distance (FID), to objectively assess image quality and realism.</li>
          <li>Provided a proof-of-concept for bridging simulation and reality in robotic vision applications.</li>
        </ul>
        
        <h5>Limitations</h5>
        <ul>
          <li>There were many challenges with generalizing across diverse viewpoints and led to geometric deformations and inaccuracies in rendering textures.</li>
        </ul>
      </div>

      <div class="thesis-images">
        <div class="project-gallery">
          <img src="images/project3-img1.png" alt="Project Image 1">
        </div>
        </div>
  </div>
  
</div>  
    